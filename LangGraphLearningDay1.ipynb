{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2I9DHlY2hDN"
      },
      "source": [
        " starting with a graph with two nodes connected by one edge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jQctIJPp2hDQ",
        "outputId": "a229d5e9-6e03-4900-fc17-e5babf686344"
      },
      "outputs": [],
      "source": [
        "#!pip install langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OBYDWDo2hDR"
      },
      "source": [
        "Nodes act like functions that can be called as needed. considering Node 1 is our starting point and Node 2 is END point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XyfaVlIa2hDR"
      },
      "outputs": [],
      "source": [
        "def function_1(input_1):\n",
        "    return input_1 + \" Hello \"\n",
        "\n",
        "def function_2(input_2):\n",
        "    return input_2 + \"how are you\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "C_19lj0t2hDS"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph\n",
        "\n",
        "flow = Graph()\n",
        "\n",
        "flow.add_node(\"node_1\", function_1)\n",
        "flow.add_node(\"node_2\", function_2)\n",
        "\n",
        "flow.add_edge('node_1', 'node_2')\n",
        "\n",
        "flow.set_entry_point(\"node_1\")\n",
        "flow.set_finish_point(\"node_2\")\n",
        "\n",
        "app = flow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Fcpv790K2hDS",
        "outputId": "0508f9ab-123e-4d32-bb96-d729ecda0c8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'vidya: Hello how are you'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke(\"vidya:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qM3kOcpY2hDS",
        "outputId": "a1f87aac-5631-4d0d-b584-f695ee72b6eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from node 'node_1':\n",
            "---\n",
            "vidya: Hello \n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'node_2':\n",
            "---\n",
            "vidya: Hello how are you\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = 'vidya:'\n",
        "for output in app.stream(input):\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQc7_T8N2hDT"
      },
      "source": [
        "# Adding LLM Call\n",
        "\n",
        "making the first node as an AGent that can call Open AI models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QcvgjLOA2hDU",
        "outputId": "e44c12fe-3d50-4726-f3cb-5316ba4f47c1"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lIT9kgWA2hDU",
        "outputId": "5232fcab-e02a-4bce-9845-f1b12ed3346f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in /home/karuturividyasagar/anaconda3/envs/genai/lib/python3.10/site-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "XjbFWxiP2hDU"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "openai_api_key= os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MYJH2bpd2hDU",
        "outputId": "0b01ce6e-8266-4623-a592-40304d72506d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/karuturividyasagar/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_community/chat_models/azure_openai.py:167: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://magicplatform-dev-canadaeast.openai.azure.com to https://magicplatform-dev-canadaeast.openai.azure.com/openai.\n",
            "  warnings.warn(\n",
            "/home/karuturividyasagar/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_community/chat_models/azure_openai.py:174: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
            "  warnings.warn(\n",
            "/home/karuturividyasagar/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_community/chat_models/azure_openai.py:182: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://magicplatform-dev-canadaeast.openai.azure.com to https://magicplatform-dev-canadaeast.openai.azure.com/openai.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6f336653-b567-45f2-9c78-cc01626f481a-0')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "\n",
        "llm_model1 = AzureChatOpenAI(\n",
        "    deployment_name=os.environ.get(\"deployment_name\"),\n",
        "    model_name=os.environ.get(\"model_name\"),\n",
        "    temperature=os.environ.get(\"temperature\"),\n",
        "    openai_api_base= os.environ.get(\"openai_api_base\"),\n",
        "    openai_api_version=os.environ.get(\"openai_api_version\"),\n",
        "    openai_api_key=os.environ.get(\"openai_api_key\"),\n",
        "    openai_api_type=os.environ.get(\"openai_api_type\"),\n",
        ")\n",
        "\n",
        "#giving user message\n",
        "llm_model1.invoke('Hey there')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzfkLICB2hDV"
      },
      "source": [
        "And if you just want to see the AI response, you can do the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QsIHP3em2hDV",
        "outputId": "641e6747-b02d-4b85-ed47-50dec64d0e60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_model1.invoke('Hey there').content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_HKaV9jQ2hDV"
      },
      "outputs": [],
      "source": [
        "def function_1(input_1):\n",
        "    response = llm_model1.invoke(input_1)\n",
        "    return response.content\n",
        "\n",
        "def function_2(input_2):\n",
        "    return \"Agent Says: \" + input_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8HTrSO4C2hDW"
      },
      "outputs": [],
      "source": [
        "#Langchain graph\n",
        "flow = Graph()\n",
        "\n",
        "#calling node 1 as agent\n",
        "flow.add_node(\"agent\", function_1)\n",
        "flow.add_node(\"node_2\", function_2)\n",
        "\n",
        "flow.add_edge('agent', 'node_2')\n",
        "\n",
        "flow.set_entry_point(\"agent\")\n",
        "flow.set_finish_point(\"node_2\")\n",
        "\n",
        "app = flow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DEV9x8hl2hDW",
        "outputId": "2f915748-0ce1-41b2-db03-1ac1d6a92a84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Agent Says: Hello! How can I assist you today?'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke(\"Hey there\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "H984a0S32hDW",
        "outputId": "ddec942a-491c-4f67-86a9-3dd21b368fff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from node 'agent':\n",
            "---\n",
            "Hello! How can I assist you today?\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'node_2':\n",
            "---\n",
            "Agent Says: Hello! How can I assist you today?\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = 'Hey there'\n",
        "for output in app.stream(input):\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE8VQx942hDX"
      },
      "source": [
        "# 1st functional Agent App - City Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def function1(input1):\n",
        "    query=\"your task is to providde onlly the city name based on the user query. \\\n",
        "        don't give anything more just give me the city name mentioned. This is the users query: \" + input1\n",
        "    response=llm_model1.invoke(query)\n",
        "    return response.content\n",
        "\n",
        "# input1=(\"what is hyderabad weather condition right now\")\n",
        "# print(function1(input1))\n",
        "\n",
        "def function2(input2):\n",
        "    return \"Agent says: \" + input2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "flow=Graph()\n",
        "flow.add_node(\"agent\", function1)\n",
        "flow.add_node(\"node2\", function2)\n",
        "flow.add_edge(\"agent\", \"node2\")\n",
        "flow.set_entry_point(\"agent\")\n",
        "flow.set_finish_point(\"node2\")\n",
        "app=flow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KYvuv4M32hDX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompiledGraph(nodes={'agent': PregelNode(config={'tags': []}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<agent>(writes=[ChannelWriteEntry(channel='agent', value=None, skip_none=False)])]), 'node2': PregelNode(config={'tags': []}, channels=['agent'], triggers=['agent'], writers=[ChannelWrite<node2>(writes=[ChannelWriteEntry(channel='node2', value=None, skip_none=False)]), ChannelWrite<__end__>(writes=[ChannelWriteEntry(channel='__end__', value=None, skip_none=False)])])}, channels={'__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbd69c0fa60>, '__end__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbd69c0f790>, 'agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbd69c0ec80>, 'node2': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbd69c3c100>}, auto_validate=False, output_channels='__end__', stream_channels=['agent', 'node2'], input_channels='__start__', graph=<langgraph.graph.graph.Graph object at 0x7fbd69c0fac0>)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "rZEY7oqx2hDX",
        "outputId": "c06dafd1-4669-4068-cf1b-4c6d024facd0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Agent says: Chennai'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke(\"What's the temperature in chennai\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVi0Wdht2hDX"
      },
      "source": [
        "\n",
        "2: Adding a weather API call\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "J2UcBJQf2hDX",
        "outputId": "3a91dcfd-2daf-4c17-dcf6-ab79e512730b"
      },
      "outputs": [],
      "source": [
        "#!pip install pyowm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "fM80gxkb2hDX"
      },
      "outputs": [],
      "source": [
        "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
        "load_dotenv()\n",
        "os.environ[\"OPENWEATHERMAP_API_KEY\"] = os.environ.get(\"OPENWEATHERMAP_API_KEY\")\n",
        "\n",
        "weather = OpenWeatherMapAPIWrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "XRJe3PB02hDX",
        "outputId": "11d3bdaa-c3ec-46b2-e470-74e8f3d3681e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In chennai, the current weather is as follows:\n",
            "Detailed status: few clouds\n",
            "Wind speed: 6.26 m/s, direction: 180°\n",
            "Humidity: 57%\n",
            "Temperature: \n",
            "  - Current: 34.7°C\n",
            "  - High: 35.0°C\n",
            "  - Low: 33.99°C\n",
            "  - Feels like: 41.7°C\n",
            "Rain: {}\n",
            "Heat index: None\n",
            "Cloud cover: 20%\n"
          ]
        }
      ],
      "source": [
        "weather_data = weather.run(\"chennai\")\n",
        "print(weather_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj_UkNwx2hDY"
      },
      "source": [
        "integrating this into function 2 and call the function two as a \"tool\" or \"weather_agent\" instead of \"node_2\" in flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "n1s26AL82hDY"
      },
      "outputs": [],
      "source": [
        "def function1(input1):\n",
        "    query=\"your task is to providde onlly the city name based on the user query. \\\n",
        "        don't give anything more just give me the city name mentioned. This is the users query: \" + input1\n",
        "    response=llm_model1.invoke(query)\n",
        "    return response.content\n",
        "\n",
        "def function2(input2):\n",
        "    weather_data = weather.run(input2)\n",
        "    return weather_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "_JaZJG5K2hDY"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph\n",
        "\n",
        "flow = Graph()\n",
        "\n",
        "#calling node 1 as agent\n",
        "flow.add_node(\"agent\", function1)\n",
        "flow.add_node(\"tool\", function2)\n",
        "\n",
        "flow.add_edge(\"agent\",\"tool\")\n",
        "\n",
        "flow.set_entry_point(\"agent\")\n",
        "flow.set_finish_point(\"tool\")\n",
        "\n",
        "app = flow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "HxouTo_D2hDY",
        "outputId": "6dfe9ed2-971a-4d87-ce51-e5b11ad46558"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In Chennai, the current weather is as follows:\\nDetailed status: few clouds\\nWind speed: 6.26 m/s, direction: 180°\\nHumidity: 57%\\nTemperature: \\n  - Current: 34.7°C\\n  - High: 35.0°C\\n  - Low: 33.99°C\\n  - Feels like: 41.7°C\\nRain: {}\\nHeat index: None\\nCloud cover: 20%'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke(\"What's the current temperature in Chennai, India\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0u4wGhgL2hDY",
        "outputId": "21727637-56ca-48c1-ad53-65cc9bf465d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from node 'agent':\n",
            "---\n",
            "Chennai\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'tool':\n",
            "---\n",
            "In Chennai, the current weather is as follows:\n",
            "Detailed status: few clouds\n",
            "Wind speed: 6.26 m/s, direction: 180°\n",
            "Humidity: 57%\n",
            "Temperature: \n",
            "  - Current: 34.7°C\n",
            "  - High: 35.0°C\n",
            "  - Low: 33.99°C\n",
            "  - Feels like: 41.7°C\n",
            "Rain: {}\n",
            "Heat index: None\n",
            "Cloud cover: 20%\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = \"What's the temperature in chennai\"\n",
        "for output in app.stream(input):\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVxJzm2c2hDY"
      },
      "source": [
        " 3 Adding another LLM Call to filter results\n",
        "\n",
        "only the temperature needed , taking another LLM call to filter data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "MhPpk5302hDY"
      },
      "outputs": [],
      "source": [
        "def function_3(input_3):\n",
        "    complete_query = \"Your task is to provide info concisely based on the user query. Following is the user query: \" + \"user input\"\n",
        "    response = llm_model1.invoke(complete_query)\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKufnAD62hDY"
      },
      "source": [
        "But the issue is the user input is not available from node 2.so should pass the user input all along from first node to the last?\n",
        "\n",
        "using dictionary and passing it between nodes (we could also use just a list, but dict is easier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "m3wQN1812hDY"
      },
      "outputs": [],
      "source": [
        "# an empty dict\n",
        "AgentState = {}\n",
        "\n",
        "# messages key will be assigned as an empty array. We will append new messages as we pass along nodes.\n",
        "AgentState[\"messages\"] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "VFOVZf1y2hDY",
        "outputId": "9c733ba8-2e73-40e6-e52b-b6d0e942dd66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': []}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AgentState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMQP4Noe2hDh"
      },
      "source": [
        "goal is to have this state filled as: {'messages': [HumanMessage, AIMessage, ...]}\n",
        "\n",
        "Also need to modify our functions to pass info along the new AgentState"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def function_1(state):\n",
        "    messages = state['messages']\n",
        "    user_input = messages[-1]\n",
        "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
        "                    Nothing more, just the city name mentioned. Following is the user query: \" + user_input\n",
        "    response = llm_model1.invoke(complete_query)\n",
        "    state['messages'].append(response.content) # appending AIMessage response to the AgentState\n",
        "    return state\n",
        "\n",
        "def function_2(state):\n",
        "    messages = state['messages']\n",
        "    agent_response = messages[-1]\n",
        "    weather = OpenWeatherMapAPIWrapper()\n",
        "    weather_data = weather.run(agent_response)\n",
        "    state['messages'].append(weather_data)\n",
        "    return state\n",
        "\n",
        "def function_3(state):\n",
        "    messages = state['messages']\n",
        "    user_input = messages[0]\n",
        "    available_info = messages[-1]\n",
        "    agent2_query = \"Your task is to provide info concisely based on the user query and the available information from the internet. \\\n",
        "                        Following is the user query: \" + user_input + \" Available information: \" + available_info\n",
        "    response = llm_model1.invoke(agent2_query)\n",
        "    return response.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "7b-RhrF22hDh"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph\n",
        "\n",
        "flow = Graph()\n",
        "\n",
        "\n",
        "flow.add_node(\"agent\", function_1)\n",
        "flow.add_node(\"tool\", function_2)\n",
        "flow.add_node(\"responder\", function_3)\n",
        "\n",
        "flow.add_edge('agent', 'tool')\n",
        "flow.add_edge('tool', 'responder')\n",
        "\n",
        "flow.set_entry_point(\"agent\")\n",
        "flow.set_finish_point(\"responder\")\n",
        "\n",
        "app = flow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "JwouXOyv2hDi",
        "outputId": "e9c488b5-f0f1-4d33-d24a-6c7387fb4fe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The current temperature in Hyderabad is 36.23°C with a few clouds. The high temperature for today is 36.73°C and the low temperature is 36.23°C. The humidity is 32% and the wind speed is 4.12 m/s with a direction of 250°. There is no rain and the heat index is none. The cloud cover is 20%.'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = {\"messages\": [\"what is the temperature in hyderabad\"]}\n",
        "app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "biSj99B22hDi",
        "outputId": "a6124224-4343-432c-a00b-7d67b594c010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from node 'agent':\n",
            "---\n",
            "{'messages': ['what is the temperature in hyderabad', 'Hyderabad']}\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'tool':\n",
            "---\n",
            "{'messages': ['what is the temperature in hyderabad', 'Hyderabad', 'In Hyderabad, the current weather is as follows:\\nDetailed status: few clouds\\nWind speed: 4.12 m/s, direction: 250°\\nHumidity: 32%\\nTemperature: \\n  - Current: 36.23°C\\n  - High: 36.73°C\\n  - Low: 36.23°C\\n  - Feels like: 37.05°C\\nRain: {}\\nHeat index: None\\nCloud cover: 20%']}\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'responder':\n",
            "---\n",
            "The current temperature in Hyderabad is 36.23°C with a few clouds. The wind speed is 4.12 m/s coming from the direction of 250°. The humidity is 32%. The high temperature for today is 36.73°C and the low temperature is 36.23°C. The heat index is not available. There is no rain at the moment and the cloud cover is 20%.\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = {\"messages\": [\"what is the temperature in hyderabad\"]}\n",
        "for output in app.stream(input):\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOqTJ7pg2hDi"
      },
      "source": [
        "\n",
        "app is not capable of answering simple questions like \"how are you?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "kpFk6Jwg2hDi",
        "outputId": "f4b8475d-6094-42ab-c706-35e1eeb08489"
      },
      "outputs": [],
      "source": [
        "# inputs = {\"messages\": [\"how are you?\"]}\n",
        "# app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GbPAbfG2hDi"
      },
      "source": [
        "This is because we always want to parse a city and then find the weather.\n",
        "\n",
        "We can make our agent smarter by saying only use the tool when needed, if not just respond back to the user.\n",
        "\n",
        "The way we can do this LangGraph is:\n",
        "1. binding a tool to the agent\n",
        "2. adding a conditional edge to the agent with the option to either call the tool or not\n",
        "3. defining the criteria for the conditional edge as when to call the tool. We will define a function for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "XpavM5fS2hDi"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGO8PjcW2hDi"
      },
      "source": [
        "Binding tool with agent (LLM Model) is made easy in langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "E2Jlxo7_2hDj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "from langchain_community.tools.openweathermap import OpenWeatherMapQueryRun\n",
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "tools = [OpenWeatherMapQueryRun()]\n",
        "\n",
        "#model = ChatOpenAI(temperature=0, streaming=True)\n",
        "functions = [convert_to_openai_function(t) for t in tools]\n",
        "model = llm_model1.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjoO84Za2hDj"
      },
      "source": [
        "Our modified function_1 now becomes as below. The reason is, we are passing the human message as state and appending response to the state. Also, our agent now has a tool bound to it, that it can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "xUKEooiQ2hDj"
      },
      "outputs": [],
      "source": [
        "def function_1(state):\n",
        "    messages = state['messages']\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": [response]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Y_2gx82hDj"
      },
      "source": [
        "For function 2, we want it to setup a tool and call it. It's made easy to invoke a tool in LangChain by using ToolInvocation and executing it with ToolExecuter. Then we respond back as a FunctionMessage so that our agent (node 1) knows that the tool was used and a response from tool is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "CoGZpKM12hDj"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tools)\n",
        "\n",
        "def function_2(state):\n",
        "    messages = state['messages']\n",
        "    last_message = messages[-1] # this has the query we need to send to the tool provided by the agent\n",
        "\n",
        "    parsed_tool_input = json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"])\n",
        "\n",
        "    # We construct an ToolInvocation from the function_call and pass in the tool name and the expected str input for OpenWeatherMap tool\n",
        "    action = ToolInvocation(\n",
        "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "        tool_input=parsed_tool_input['__arg1'],\n",
        "    )\n",
        "\n",
        "    # We call the tool_executor and get back a response\n",
        "    response = tool_executor.invoke(action)\n",
        "\n",
        "    # We use the response to create a FunctionMessage\n",
        "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjVgnI8L2hDj"
      },
      "source": [
        "Finally, we define a function for the conditional edge, to help us figure out which direction to go (tool or user response)\n",
        "\n",
        "We can benefit from the agent (LLM) response in LangChain, which has additional_kwargs to make a function_call with the name of the tool.\n",
        "\n",
        "So our logic is, if function_call available in the additional_kwargs, then call tool if not then end the discussion and respond back to the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "I25WG4pU2hDj"
      },
      "outputs": [],
      "source": [
        "def where_to_go(state):\n",
        "    messages = state['messages']\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    if \"function_call\" in last_message.additional_kwargs:\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"end\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqSkQu4k2hDj"
      },
      "source": [
        "Now with all of the changes above, our LangGraph app is modified as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "jOuiOTjd2hDj"
      },
      "outputs": [],
      "source": [
        "# from langgraph.graph import Graph, END\n",
        "\n",
        "# workflow = Graph()\n",
        "\n",
        "# Or you could import StateGraph and pass AgentState to it\n",
        "from langgraph.graph import StateGraph, END\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", function_1)\n",
        "workflow.add_node(\"tool\", function_2)\n",
        "\n",
        "# The conditional edge requires the following info below.\n",
        "# First, we define the start node. We use `agent`.\n",
        "# This means these are the edges taken after the `agent` node is called.\n",
        "# Next, we pass in the function that will determine which node is called next, in our case where_to_go().\n",
        "\n",
        "workflow.add_conditional_edges(\"agent\", where_to_go,{   # Based on the return from where_to_go\n",
        "                                                        # If return is \"continue\" then we call the tool node.\n",
        "                                                        \"continue\": \"tool\",\n",
        "                                                        # Otherwise we finish. END is a special node marking that the graph should finish.\n",
        "                                                        \"end\": END\n",
        "                                                    }\n",
        ")\n",
        "\n",
        "# We now add a normal edge from `tools` to `agent`.\n",
        "# This means that if `tool` is called, then it has to call the 'agent' next.\n",
        "workflow.add_edge('tool', 'agent')\n",
        "\n",
        "# Basically, agent node has the option to call a tool node based on a condition,\n",
        "# whereas tool node must call the agent in all cases based on this setup.\n",
        "\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkxvogbS2hDk"
      },
      "source": [
        "We also pass the first message using HumanMessage component available in langchain, makes it easy to differentiate from AIMessage, and FunctionMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "uaBlYTCx2hDk",
        "outputId": "f3d7aa87-3992-4e5a-827f-f72d15316078"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is the temperature in las vegas'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"__arg1\": \"Las Vegas\"\\n}', 'name': 'open_weather_map'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 82, 'total_tokens': 100}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-4a739a35-b856-4986-9501-db4a2f2db105-0'),\n",
              "  FunctionMessage(content='In Las Vegas, the current weather is as follows:\\nDetailed status: broken clouds\\nWind speed: 5.81 m/s, direction: 230°\\nHumidity: 19%\\nTemperature: \\n  - Current: 24.4°C\\n  - High: 25.45°C\\n  - Low: 23.24°C\\n  - Feels like: 23.39°C\\nRain: {}\\nHeat index: None\\nCloud cover: 75%', name='open_weather_map'),\n",
              "  AIMessage(content='The current temperature in Las Vegas is 24.4°C.', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 207, 'total_tokens': 221}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2635e9f7-8239-47e5-9adc-ba546ddbe3f1-0')]}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas\")]}\n",
        "app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "o89ayUP12hDk",
        "outputId": "3a54b7fe-2133-4fd1-d60b-18e11056c461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from node 'agent':\n",
            "---\n",
            "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"__arg1\": \"Las Vegas\"\\n}', 'name': 'open_weather_map'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 82, 'total_tokens': 100}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-c2597e7e-0143-4ba0-88e5-3350a59efdf8-0')]}\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'tool':\n",
            "---\n",
            "{'messages': [FunctionMessage(content='In Las Vegas, the current weather is as follows:\\nDetailed status: broken clouds\\nWind speed: 5.81 m/s, direction: 230°\\nHumidity: 19%\\nTemperature: \\n  - Current: 24.4°C\\n  - High: 25.45°C\\n  - Low: 23.24°C\\n  - Feels like: 23.39°C\\nRain: {}\\nHeat index: None\\nCloud cover: 75%', name='open_weather_map')]}\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'agent':\n",
            "---\n",
            "{'messages': [AIMessage(content='The current temperature in Las Vegas is 24.4°C.', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 207, 'total_tokens': 221}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8fc2762c-8078-477a-9b1e-a602641fcab6-0')]}\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas\")]}\n",
        "for output in app.stream(inputs):\n",
        "    # stream() yields dictionaries with output keyed by node name\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp5z2ryk2hDk"
      },
      "source": [
        "Hopefully, that gives you a good understanding of how we built a LangGraph app and why we used different LC components."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
